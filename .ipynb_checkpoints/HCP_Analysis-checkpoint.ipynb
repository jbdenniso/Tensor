{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/strange/.local/lib/python3.7/site-packages/nilearn/datasets/__init__.py:90: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  \"Numpy arrays.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Import a bunch of Libraries and define some nice functions\n",
    "import sklearn as skl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sms\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "from scipy.stats import zscore\n",
    "from nilearn import datasets,image,masking,signal,plotting,input_data\n",
    "from natsort import natsorted \n",
    "from IPython.core.display import HTML\n",
    "def multi_table(table_list):\n",
    "    return HTML(\n",
    "    '<table><tr style=\"background-color:white;\">'+\n",
    "    ''.join(['<td>'+table._repr_html_()+'</td>' for table in table_list])+\n",
    "    '</tr></table>'\n",
    "    )\n",
    "def eta_squared(aov):\n",
    "    aov['eta_sq'] = 'NaN'\n",
    "    aov['eta_sq'] = aov[:-1]['sum_sq']/sum(aov['sum_sq'])\n",
    "    return aov\n",
    "\n",
    "def omega_squared(aov):\n",
    "    mse = aov['sum_sq'][-1]/aov['df'][-1]\n",
    "    aov['omega_sq'] = 'NaN'\n",
    "    aov['omega_sq'] = (aov[:-1]['sum_sq']-(aov[:-1]['df']*mse))/(sum(aov['sum_sq'])+mse)\n",
    "    return aov\n",
    "\n",
    "from patsy.contrasts import ContrastMatrix\n",
    "\n",
    "def _name_levels(prefix, levels):\n",
    "    return [\"[%s%s]\" % (prefix, level) for level in levels]\n",
    "\n",
    "class Simple(object):\n",
    "    def _simple_contrast(self, levels):\n",
    "        nlevels = len(levels)\n",
    "        contr = -1./nlevels * np.ones((nlevels, nlevels-1))\n",
    "        contr[1:][np.diag_indices(nlevels-1)] = (nlevels-1.)/nlevels\n",
    "        return contr\n",
    "\n",
    "    def code_with_intercept(self, levels):\n",
    "        contrast = np.column_stack((np.ones(len(levels)),\n",
    "                                    self._simple_contrast(levels)))\n",
    "        return ContrastMatrix(contrast, _name_levels(\"Simp.\", levels))\n",
    "\n",
    "    def code_without_intercept(self, levels):\n",
    "        contrast = self._simple_contrast(levels)\n",
    "        return ContrastMatrix(contrast, _name_levels(\"Simp.\", levels[:-1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'all_DesignMatrix.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1b3e51603806>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Import Subject Data and ICA outputfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msub_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all_DesignMatrix.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msub_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Subject'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'sub-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msub_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Subject'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'str'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mMelodic_Outs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnatsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tica'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.ica'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    934\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1996\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1998\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1999\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'all_DesignMatrix.csv'"
     ]
    }
   ],
   "source": [
    "#Import Subject Data and ICA outputfiles\n",
    "sub_data=pd.read_csv('all_DesignMatrix.csv',index_col=0)\n",
    "sub_data['Subject'] = 'sub-' + sub_data['Subject'].astype('int').astype('str')\n",
    "Melodic_Outs=natsorted([d for d in os.listdir('.') if ((d.startswith('tica')) & (d.endswith('.ica')))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make the 3 nilearn images for masks\n",
    "from nilearn import image,datasets,masking,plotting,input_data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "HO_sub=datasets.fetch_atlas_harvard_oxford(atlas_name='sub-maxprob-thr50-2mm')\n",
    "keep = [idx for idx, s in enumerate(HO_sub.labels) if\n",
    "        'White' in s or 'Ventricle' in s or 'Ventrical' in s or '']\n",
    "display(keep)\n",
    "noise_regions=image.concat_imgs(np.array(\n",
    "    [image.math_img(\"img==1\", img=HO_sub.maps),image.math_img(\"img==3\", img=HO_sub.maps),\n",
    "     image.math_img(\"img==12\", img=HO_sub.maps),image.math_img(\"img==14\", img=HO_sub.maps)]))\n",
    "                                 \n",
    "str_regions=image.concat_imgs(np.array(\n",
    "    [image.math_img(\"img==5\", img=HO_sub.maps),image.math_img(\"img==6\",img=HO_sub.maps),\n",
    "     image.math_img(\"img==7\", img=HO_sub.maps),image.math_img(\"img==11\", img=HO_sub.maps),\n",
    "     image.math_img(\"img==16\", img=HO_sub.maps),image.math_img(\"img==17\", img=HO_sub.maps),\n",
    "     image.math_img(\"img==18\", img=HO_sub.maps),image.math_img(\"img==22\", img=HO_sub.maps)]))\n",
    "\n",
    "\n",
    "WM_mask=masking.compute_background_mask(image.index_img(noise_regions,[0,2]))\n",
    "Vent_mask=masking.compute_background_mask(image.index_img(noise_regions,[1,3]))\n",
    "str_mask=masking.compute_background_mask(str_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Mask Figures to show areas Included\n",
    "if 1==1:\n",
    "    fig, axs = plt.subplots(3, 2,figsize=(16, 16))\n",
    "    IC_image=image.load_img(Melodic_Outs[0]+'/stats/thresh_zstat1.nii.gz')\n",
    "\n",
    "    plotting.plot_roi(WM_mask,axes=axs[0][0])\n",
    "    plotting.plot_roi(Vent_mask,axes=axs[1][0])\n",
    "    plotting.plot_roi(str_mask,axes=axs[2][0])\n",
    "    axs[0][0].set_title('White Matter')\n",
    "    axs[1][0].set_title('Ventricles')\n",
    "    axs[2][0].set_title('Striatum')\n",
    "\n",
    "    wm_masker=input_data.NiftiMasker(mask_img=WM_mask)\n",
    "    maskImage=wm_masker.fit_transform(IC_image)\n",
    "    plotting.plot_glass_brain(wm_masker.inverse_transform(maskImage),axes=axs[0][1])\n",
    "\n",
    "    vent_masker=input_data.NiftiMasker(mask_img=Vent_mask)\n",
    "    maskImage=vent_masker.fit_transform(IC_image)\n",
    "    plotting.plot_glass_brain(vent_masker.inverse_transform(maskImage),axes=axs[1][1])\n",
    "\n",
    "    str_masker=input_data.NiftiMasker(mask_img=str_mask)\n",
    "    maskImage=str_masker.fit_transform(IC_image)\n",
    "    plotting.plot_glass_brain(str_masker.inverse_transform(maskImage),axes=axs[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find Percent of above threshold voxels for White Matter, Ventricles, and Striatum for each IC\n",
    "# Import Manually Classified IC's\n",
    "man_class=[]\n",
    "for file in [file for file in os.listdir('.') if ((file.startswith('Manual')&(file.endswith('.csv'))))]:\n",
    "    print(file)\n",
    "    t=pd.read_csv(file)\n",
    "    t.rename(columns = {\"Component\": \"IC\"},  \n",
    "          inplace = True) \n",
    "    #t['file']=(re.search('(.*)_noCov.csv' ,file).group(1))\n",
    "    task=re.search('task-(.*?).csv',file).group(1)\n",
    "    ds='tica_00dim_task-%s_LindseySubs_unsmoothed_merged_z.ica'%(task)\n",
    "    \n",
    "    f_list=natsorted([ ds+'/stats/'+f for f in os.listdir(ds+'/stats') if f.startswith('thresh')])\n",
    "    \n",
    "    \n",
    "    row=[]\n",
    "    for i,f in enumerate(f_list):\n",
    "        IC='IC%02d'%(int(re.search('zstat(.*).nii',f).group(1)))\n",
    "        p_map=ds+'/stats/probmap_%s.nii.gz'%(int(re.search('zstat(.*).nii',f).group(1)))\n",
    "        #row.append([ds,task,run,IC,f,p_map])\n",
    "\n",
    "        #print(f,IC)\n",
    "        wm=wm_masker.fit_transform(f)\n",
    "        vent=vent_masker.fit_transform(f)\n",
    "        str_=str_masker.fit_transform(f)\n",
    "        row.append([ds,task,IC,f,p_map,\n",
    "        np.divide(np.count_nonzero(np.where(np.absolute(wm)>1)),wm.shape[1]),\n",
    "        np.divide(np.count_nonzero(np.where(np.absolute(vent)>1)),vent.shape[1]),\n",
    "        np.divide(np.count_nonzero(np.where(np.absolute(str_)>1)),str_.shape[1])])\n",
    "        \n",
    "    IC_df=pd.DataFrame(columns=['directory','task','run','IC','file','prob_map','WhiteMatter','Ventricle','Striatum'],data=row)\n",
    "    #IC_df=pd.DataFrame(columns=['directory','task','run','IC','file','prob_map'],data=row)\n",
    "    display(IC_df.head())\n",
    "    t=t.merge(IC_df, on='IC')\n",
    "    man_class.append(t)\n",
    "    \n",
    "man_class=pd.concat(man_class)\n",
    "man_class.reset_index(drop=True,inplace=True)\n",
    "man_class['One_IsGood']=man_class['J_IsGood']+man_class['D_IsGood']>0\n",
    "\n",
    "\n",
    "#test=IC_df[(IC_df['Striatum']>IC_df['Ventricle'])&(IC_df['Striatum']>IC_df['WhiteMatter'])|(IC_df['IC'].apply(str).str.contains('40'))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Melodic_Outs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0045e8a298cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnibabel\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s/mask.nii.gz\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMelodic_Outs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m#plotting.plot_roi(mask)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Melodic_Outs' is not defined"
     ]
    }
   ],
   "source": [
    "from nilearn.plotting import plot_prob_atlas\n",
    "from nilearn import image\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "\n",
    "mask=image.load_img(\"%s/mask.nii.gz\"%(Melodic_Outs[0]))\n",
    "#plotting.plot_roi(mask)\n",
    "\n",
    "mask=input_data.NiftiMasker(mask_img=masking.compute_background_mask(mask))\n",
    "\n",
    "\n",
    "\n",
    "for task in man_class['task'].unique():\n",
    "    for run in man_class['run'].unique():\n",
    "        display(task,run)\n",
    "        df=man_class[(man_class['task']==task) & (man_class['run']==run) & (man_class['Both Agree']==True)]\n",
    "        #plot_prob_atlas(df['prob_map'],cut_coords=[0,5,5],threshold=0.999,draw_cross=False)\n",
    "        #plt.show()\n",
    "        img_list=mask.fit_transform(df['prob_map'])\n",
    "        img_abs=np.sum(np.absolute(img_list)>0.95,axis=0)\n",
    "        nib.save(mask.inverse_transform(img_abs),\"Overlap_task-%s_run-%s.nii.gz\"%(task,run))\n",
    "        display(plotting.view_img(mask.inverse_transform(img_abs),\n",
    "                          threshold=2, title=\"Overlap\",draw_cross=False,cmap='gist_ncar',symmetric_cmap=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for results in man_class.directory.unique():\n",
    "    print(results)\n",
    "    good_ICS=man_class[(man_class['directory']==results)&(man_class['Both Agree']==True)]['IC']\n",
    "    log_file = open(results+'/log.txt', 'r') \n",
    "    Lines = log_file.readlines()\n",
    "    log_file.close()\n",
    "    \n",
    "    task=re.search('task-(.*)_r',results).group(1)\n",
    "    run=re.search('run-(.*)_L',results).group(1)\n",
    "    \n",
    "    subs=[]\n",
    "    for line in Lines:\n",
    "        if line.startswith(\"Excluding voxels with constant value\"):\n",
    "            break\n",
    "        elif line.startswith(\"Reading data file\"):\n",
    "            subs.append('sub-'+re.search('sub-(.*)_task',line).group(1))\n",
    "            \n",
    "    smodes=pd.read_csv(results+'/melodic_Smodes',\n",
    "                       delim_whitespace=True,header=None)\n",
    "\n",
    "    colnames=['IC%02d'%(i+1) for i,col in enumerate(smodes.columns)]\n",
    "    smodes.columns=colnames\n",
    "    smodes=smodes[good_ICS]\n",
    "    \n",
    "    smodes['Subject']=subs\n",
    "    smodes.to_csv('ForPalm_task-%s_run-%s.csv'%(task,run))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from patsy.contrasts import Poly\n",
    "\n",
    "sms.set_theme(style=\"whitegrid\")\n",
    "\n",
    "for results in man_class.directory.unique():\n",
    "    good_ICS=man_class[(man_class['directory']==results)&(man_class['Both Agree']==True)]['IC']\n",
    "\n",
    "    print(results)\n",
    "    log_file = open(results+'/log.txt', 'r') \n",
    "    Lines = log_file.readlines()\n",
    "    log_file.close()\n",
    "    \n",
    "    task=re.search('task-(.*)_r',results).group(1)\n",
    "    run=re.search('run-(.*)_L',results).group(1)\n",
    "    \n",
    "    subs=[]\n",
    "    for line in Lines:\n",
    "        if line.startswith(\"Excluding voxels with constant value\"):\n",
    "            break\n",
    "        elif line.startswith(\"Reading data file\"):\n",
    "            subs.append('sub-'+re.search('sub-(.*)_task',line).group(1))\n",
    "            \n",
    "    smodes=pd.read_csv(results+'/melodic_Smodes',\n",
    "                       delim_whitespace=True,header=None)\n",
    "\n",
    "    colnames=['IC%02d'%(i+1) for i,col in enumerate(smodes.columns)]\n",
    "    smodes.columns=colnames\n",
    "    smodes=smodes[good_ICS]\n",
    "    smodes['Subject']=subs\n",
    "\n",
    "    #smodes.to_csv('ForPalm_task-%s_run-%s.csv'%(task,run))\n",
    "    smodes=pd.merge(smodes, sub_data, on='Subject')\n",
    "    #smodes=pd.merge(smodes, s_ab, on='Subject')\n",
    "    smodes['groups']=smodes[['Dep','FamHist','HC']].idxmax(axis=1)\n",
    "    smodes.rename(columns = {'40K_demeaned':'delay'}, inplace = True)\n",
    "    for var in['Dep_Sx_demeaned','Sadness_demeaned','Drinks_demeaned', 'Tobacco_demeaned']:\n",
    "        smodes[\"log_%s\"%(var)]=np.sqrt(smodes[\"%s\"%(var)])\n",
    "\n",
    "    \n",
    "    Adj_p=np.divide(0.05,len(man_class[(man_class['Both Agree']==True)&(man_class['directory']==results)]))\n",
    "    print(\"Adjusted alpha: %s\"%Adj_p)\n",
    "                                        \n",
    "    for IC in colnames:\n",
    "        if man_class[(man_class['IC']==IC)&(man_class['directory']==results)]['Both Agree'].values[0]:\n",
    "            \n",
    "            levels = smodes.groups.unique().tolist()\n",
    "            contrast = Poly().code_without_intercept(levels)\n",
    "            \n",
    "            res = smodes.groupby(\"groups\")[IC].quantile([.25, .75]).unstack(level=1)\n",
    "            res['IQR']=res.iloc[:,1]-res.iloc[:,0]\n",
    "            res['3_IQR']=res['IQR']*1.5\n",
    "            res['mean']=smodes.groupby(\"groups\")[IC].mean()\n",
    "            res['upper']=res['mean']+res['3_IQR']\n",
    "            res['lower']=res['mean']-res['3_IQR']\n",
    "\n",
    "            smodes=smodes.loc[((res.loc[smodes.groups, 'lower'] < smodes[IC].values)\n",
    "                        & (smodes[IC].values < res.loc[smodes.groups, 'upper'])).values]\n",
    "            \n",
    "            mod = ols('%s ~ groups'%(IC),data=smodes).fit()\n",
    "            \n",
    "            #aov_table = sm.stats.anova_lm(mod, typ=2)\n",
    "\n",
    "            if mod.f_pvalue<0.05:\n",
    "                print(results)\n",
    "                #display(mod.summary2())\n",
    "                #aov_table = sm.stats.anova_lm(mod, typ=2)\n",
    "                display(aov_table)\n",
    "                pair_t = mod.t_test_pairwise('groups')\n",
    "                display(pair_t.result_frame)\n",
    "                #Plot\n",
    "                dirname='%s_%s_%s'%(task,run,IC)\n",
    "                os.makedirs(dirname,exist_ok=True)\n",
    "                sms.barplot(x='groups',y=IC,data=smodes,ci=68,capsize=.05,\n",
    "                            order=['Dep','FamHist','HC'],palette=\"PuBuGn_d\")\n",
    "                plt.savefig(dirname+'/%s_groups.svg'%(IC))\n",
    "                plt.show()\n",
    "\n",
    "                #fig, axs = plt.subplots(2, 2,figsize=(16, 16))\n",
    "\n",
    "                #for i,var in enumerate(['Dep_Sx_demeaned','Sadness_demeaned','delay',\n",
    "                                        #'Drinks_demeaned', 'Tobacco_demeaned']):\n",
    "                    #sms.lmplot(x=var,y=IC,data=smodes)\n",
    "                    #plt.savefig(dirname+'/%s_%s.svg'%(IC,var))                    \n",
    "                    #plt.show()\n",
    "                    \n",
    "                    #mod = ols('%s ~ %s'%(IC,var),data=smodes[smodes['groups']=='Dep']).fit()\n",
    "                    #if mod.pvalues[var]<0.05:\n",
    "                        #display(mod.summary())\n",
    "                #for i,var in enumerate(['Drinks_demeaned', 'Tobacco_demeaned']):\n",
    "                    #ms.regplot(x=var,y=IC,data=smodes_plot,ax=axs[1][i])\n",
    "\n",
    "                #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for task in man_class['task'].unique():\n",
    "    for run in man_class['run'].unique():\n",
    "        display(task,run)\n",
    "        df=man_class[(man_class['task']==task) & (man_class['run']==run) &\n",
    "                     (man_class['IC'].isin(['IC02','IC03','IC05','IC06','IC07']))]\n",
    "        #plot_prob_atlas(df['prob_map'],cut_coords=[0,5,5],threshold=0.999,draw_cross=False)\n",
    "        #plt.show()\n",
    "        img_list=mask.fit_transform(df['prob_map'])\n",
    "        img_abs=np.sum(np.absolute(img_list)>0.95,axis=0)\n",
    "        nib.save(mask.inverse_transform(img_abs),\"Overlap_task-%s_run-%s.nii.gz\"%(task,run))\n",
    "        display(plotting.view_img(mask.inverse_transform(img_abs),\n",
    "                          threshold=2, title=\"Overlap\",draw_cross=False,cmap='gist_ncar',symmetric_cmap=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for results in man_class.directory.unique():\n",
    "    good_ICS=man_class[(man_class['directory']==results)&(man_class['Both Agree']==True)]['IC']\n",
    "    \n",
    "    design=pd.read_csv('L1_task-Gam_run-RL_design.mat',skiprows=5,sep='\\t',\n",
    "                   header=None,names=['Gains','Losses','DropThis'])\n",
    "    design=design[['Gains','Losses']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "task='Gam'\n",
    "run='RL'\n",
    "\n",
    "design=pd.read_csv('L1_task-%s_run-%s_design.mat'%(task,run),skiprows=5,sep='\\t',\n",
    "                   header=None,names=['Gains','Losses','DropThis'])\n",
    "design=design[['Gains','Losses']]\n",
    "#read the participants csv file so you know what group they were in and demographics\n",
    "sub_data['groups']=sub_data[['Dep','FamHist','HC']].idxmax(axis=1)\n",
    "\n",
    "df_list=[]\n",
    "\n",
    "for row in sub_data.iterrows():\n",
    "    #display(row[1])\n",
    "    tdf=design\n",
    "    tdf['sub']=row[1]['Subject']\n",
    "    tdf['group']=row[1]['groups']\n",
    "    df_list.append(tdf.copy())\n",
    "   \n",
    "df=pd.concat(df_list).reset_index(drop=True)\n",
    "data_dir='tica_00dim_task-%s_run-%s_LindseySubs_wDesigns_new_unsmoothed.ica/report/'%(task,run)\n",
    "files=[data_dir+f for f in os.listdir(data_dir) if f.startswith('t') if f.endswith('.txt')]\n",
    "\n",
    "for f in natsorted(files):\n",
    "\n",
    "    Tweight_df=pd.read_csv(f,delim_whitespace=True,\n",
    "                           header=None,names=subs)\n",
    "    #display(Tweight_df)\n",
    "    Tweight_df.reset_index(drop=True,inplace=True)\n",
    "    #display(Tweight_df.mean(axis=1))\n",
    "    \n",
    "    Zweight_df=Tweight_df.apply(zscore)\n",
    "    \n",
    "    df[\"IC%02d\"%(int(re.search('report/t(.*).txt',f).group(1)))]=Zweight_df.melt()['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.formula.api import ols,mixedlm,gls\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "import statsmodels\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import natsort\n",
    "\n",
    "data=[]\n",
    "good_ICS=man_class[(man_class['Both Agree']==True)&\n",
    "    (man_class[\n",
    "        'directory']=='tica_00dim_task-%s_run-%s_LindseySubs_wDesigns_new_unsmoothed.ica'%(\n",
    "    task,run))]['IC']\n",
    "#for IC in IC_df[(IC_df['Striatum']>IC_df['Ventricle'])& (IC_df['Striatum']>IC_df['WhiteMatter'])|(IC_df['IC'].apply(str).str.contains('40'))].IC:\n",
    "for IC in [IC for IC in good_ICS]:\n",
    "    \n",
    "    row=[]\n",
    "    for sub in natsorted(df['sub'].unique()):\n",
    "        e1=\"%s ~ Gains + Losses\"%(IC)\n",
    "        t_df=df[df['sub']==sub]\n",
    "        model1 = ols(e1, t_df).fit()\n",
    "\n",
    "        row.append([sub,t_df.group.unique()[0],\n",
    "                    IC,model1.params['Gains'],model1.params['Losses']])\n",
    "    \n",
    "    row=np.array(row)\n",
    "    \n",
    "    data.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reg_df=pd.DataFrame(columns=['sub','group','IC','Gains','Losses'],\n",
    "                    data=np.concatenate(data))\n",
    "reg_df['Gains']=pd.to_numeric(reg_df['Gains'])\n",
    "reg_df['Losses']=pd.to_numeric(reg_df['Losses'])\n",
    "\n",
    "reg_df['Diff']=reg_df['Gains']-reg_df['Losses']\n",
    "#reg_df=pd.melt(reg_df, id_vars=['sub','group','IC'], value_vars=['Gains','Losses','Diff'])\n",
    "#reg_df['value']=pd.to_numeric(reg_df['value'])\n",
    "reg_df.head()\n",
    "\n",
    "pivot_df=reg_df[['sub','IC','Diff']]\n",
    "pivot_df=pivot_df.pivot('sub','IC')\n",
    "pivot_df.to_csv('Fit_T_beta_diff_task-%s_run-%s.csv'%(task,run))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_df=reg_df[reg_df['variable']!='Diff'].copy()\n",
    "sms.set_theme(style=\"whitegrid\")\n",
    "\n",
    "for IC in reg_df.IC.unique():\n",
    "    print(IC)\n",
    "    tdf=plot_df[plot_df['IC']==IC]\n",
    "    model = ols('value ~ C(group)',\n",
    "                data=tdf).fit()\n",
    "    #display(model.summary())\n",
    "    display(sm.stats.anova_lm(model, typ=1))    \n",
    "    \n",
    "\n",
    "    #sms.barplot(x='group', y='value', hue='variable',ci=68,\n",
    "                #data=plot_df[plot_df['IC']==IC])\n",
    "    g = sms.catplot(x=\"variable\", y=\"value\", hue=\"group\", \n",
    "                capsize=.2,hue_order=['Dep','FamHist','HC'],palette=\"PuBuGn_d\", height=6, aspect=.75,\n",
    "                kind=\"point\", data=tdf)\n",
    "    plt.legend(bbox_to_anchor=(1.01, 1),\n",
    "           borderaxespad=0)\n",
    "    plt.savefig('Gain_Loss_%s.svg'%(IC))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in reg_df.group.unique():\n",
    "        print(group)\n",
    "        ttdf=tdf[tdf['group']==group]\n",
    "        mod = ols('value ~ variable',data=ttdf).fit()\n",
    "        pair_t = mod.t_test_pairwise('variable')\n",
    "        display(pair_t.result_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "ttdf=tdf[tdf['group']=='Dep']\n",
    "# Python paired sample t-test\n",
    "ttest_rel(ttdf[ttdf['variable']=='Gains'].value, \n",
    "         ttdf[ttdf['variable']=='Losses'].value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
